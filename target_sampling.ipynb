{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig\n",
    "import gymnasium as gym\n",
    "\n",
    "from kitten.common.rng import global_seed, Generator\n",
    "from kitten.policy import Policy\n",
    "from kitten.experience.collector import GymCollector\n",
    "from kitten.experience.util import build_transition_from_list, build_replay_buffer\n",
    "from kitten.experience import Transitions, AuxiliaryMemoryData\n",
    "from kitten.rl.common import td_lambda\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cats.agent.minigrid.value import MinigridValue\n",
    "from cats.agent.experiment import ExperimentBase, build_env\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def visualise(env, batch: Transitions | list[np.ndarray]):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    draw_map = np.zeros(obs.shape)\n",
    "    white = np.ones(obs.shape)\n",
    "    red = np.zeros(obs.shape)\n",
    "    red[:, :, 0] = 1\n",
    "    green = np.zeros(obs.shape)\n",
    "    green[:, :, 1] = 1\n",
    "    blue = np.zeros(obs.shape)\n",
    "    blue[:, :, 2] = 1\n",
    "\n",
    "    # Draw walls\n",
    "    has_wall = np.expand_dims((obs[:, :, 0] == 2), axis=-1)\n",
    "    draw_map += white * has_wall * 0.8\n",
    "    # Draw goal\n",
    "    has_wall = np.expand_dims((obs[:, :, 0] == 8), axis=-1)\n",
    "    draw_map += green * has_wall * 1\n",
    "\n",
    "    # Draw agent exploration\n",
    "    if isinstance(batch, Transitions):\n",
    "        states = batch.s_0.detach().cpu().numpy()  # [batch, a, b, c]\n",
    "    else:\n",
    "        states = np.array(batch)\n",
    "    has_agent = states[:, :, :, 0] == 10\n",
    "    has_agent = np.sum(has_agent, axis=0)\n",
    "    #has_agent = np.maximum(0, np.log(has_agent))\n",
    "    has_agent = np.expand_dims(has_agent / has_agent.max(), axis=-1)\n",
    "    draw_map += red * has_agent\n",
    "\n",
    "    plt.imshow(draw_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationPolicy(Policy):\n",
    "    \"\"\"Purely Random Exploration\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, env: gym.Env, rng: Generator, repeat_probability: float = 0.9\n",
    "    ) -> None:\n",
    "        super().__init__(fn=None)\n",
    "        self.action_space = copy.deepcopy(env.action_space)\n",
    "        self.action_space.seed(int(rng.numpy.integers(2**32 - 1)))\n",
    "        self.rng = rng\n",
    "        self.p = repeat_probability\n",
    "        self.previous_action = None\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        if self.previous_action is None or self.rng.numpy.random() > self.p:\n",
    "            self.previous_action = self.action_space.sample()\n",
    "            return self.previous_action\n",
    "        else:\n",
    "            return self.previous_action\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.previous_action = None\n",
    "\n",
    "\n",
    "class TeleportStrategyExperiment(ExperimentBase):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig,\n",
    "        deprecated_testing_flag: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__(cfg,\n",
    "                         normalise_obs=False,\n",
    "                         deprecated_testing_flag=False,\n",
    "                         device=device)\n",
    "        self._gamma: float = cfg.algorithm.gamma\n",
    "        self._lmbda: float = cfg.algorithm.lmbda\n",
    "\n",
    "\n",
    "    def _build_policy(self) -> None:\n",
    "        # Random Policy\n",
    "        self._policy = ExplorationPolicy(\n",
    "            self.env, self.rng.build_generator(), repeat_probability=self.cfg.policy.p\n",
    "        )\n",
    "        # Build Value Estimator\n",
    "        self.value = MinigridValue().to(DEVICE)\n",
    "        self.optim_v = torch.optim.Adam(params=self.value.parameters()) \n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self._policy\n",
    "    \n",
    "    @property\n",
    "    def value_container(self):\n",
    "        return self.value\n",
    "\n",
    "    def run(self):\n",
    "        self.tm.reset(self.collector.env, self.collector.env)\n",
    "        step, steps = 0, self.cfg.train.total_frames\n",
    "        while step < steps:\n",
    "            \n",
    "            # Collect batch\n",
    "            batch_ = []\n",
    "            goal_step = step + 100 # TODO\n",
    "            while step < goal_step:\n",
    "                step += 1\n",
    "                data = self.collector.collect(n=1)[-1]\n",
    "                batch_.append(data)\n",
    "                self.tm.update(self.collector.env, obs=data[0])\n",
    "                if data[-2] or data[-1]: # Terminated or truncated\n",
    "                    break\n",
    "            batch = build_transition_from_list(batch_, device=DEVICE)\n",
    "\n",
    "            # Reshape for ConvNet\n",
    "            batch.s_0 = batch.s_0.permute(0, 3, 1, 2)\n",
    "            batch.s_1 = batch.s_1.permute(0, 3, 1, 2)\n",
    "\n",
    "            # Intrinsic Update\n",
    "            for _ in range(4):\n",
    "                self.intrinsic.update(batch, aux=AuxiliaryMemoryData.placeholder(batch), step=step)\n",
    "            \n",
    "            # Override rewards\n",
    "            _, _, r_i = self.intrinsic.reward(batch)\n",
    "            batch.r = r_i\n",
    "            value_targets = td_lambda(batch, self._lmbda, self._gamma, self.value)\n",
    "            total_value_loss = 0\n",
    "            for _ in range(4):\n",
    "                self.optim_v.zero_grad()\n",
    "                value_loss = ((value_targets - self.value.v(batch.s_0)) ** 2).mean()\n",
    "                total_value_loss += value_loss.item()\n",
    "                value_loss.backward()\n",
    "                self.optim_v.step()\n",
    "\n",
    "            # Always reset? Or only reset on truncation?\n",
    "            self._reset(batch_[-1][0], batch_[-1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_random(steps: int = 1000, seed: int = 0):\n",
    "    with initialize(version_base=None, config_path=\"cats/config\"):\n",
    "        cfg = compose(\n",
    "            config_name=\"defaults_online.yaml\",\n",
    "            overrides=[\n",
    "                f\"seed={seed}\",\n",
    "                \"cats.fixed_reset=true\",\n",
    "                \"cats.teleport.enable=true\",\n",
    "            ],\n",
    "        )\n",
    "    env = build_env(cfg)\n",
    "    rng = global_seed(seed=seed)\n",
    "    policy = ExplorationPolicy(env, rng=rng.build_generator(), repeat_probability=0.5)\n",
    "    collector = GymCollector(policy, env)\n",
    "    batch = build_transition_from_list(collector.collect(steps), DEVICE)\n",
    "    return env, batch\n",
    "\n",
    "\n",
    "def experiment_teleport(steps: int = 10000, seed: int = 0):\n",
    "    with initialize(version_base=None, config_path=\"cats/config\"):\n",
    "        cfg = compose(\n",
    "            config_name=\"defaults_online.yaml\",\n",
    "            overrides=[\n",
    "                f\"seed={seed}\",\n",
    "                f\"train.total_frames={steps}\",\n",
    "                \"cats.fixed_reset=true\",\n",
    "                \"cats.teleport.enable=true\",\n",
    "                f\"env.max_steps={steps}\"\n",
    "            ],\n",
    "        )\n",
    "    experiment = TeleportStrategyExperiment(cfg, device=DEVICE)\n",
    "    experiment.run()    \n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiment_teleport(steps=10000)\n",
    "env, batch = experiment.env, experiment.memory.sample(len(experiment.memory))[0]\n",
    "visualise(env, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(experiment.env, batch=experiment.logger._engine.results['reset_obs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(env):\n",
    "    obs, _ = env.reset()\n",
    "    has_agent = obs[:, :, 0] == 10\n",
    "    obs[:, :, 0] -= (has_agent * 9).astype(np.uint8)\n",
    "    grid = np.reshape(obs, (1,1, *obs.shape))\n",
    "    grid = np.repeat(grid, len(obs), 0)\n",
    "    grid = np.repeat(grid, len(obs), 1)\n",
    "    for x in range(len(obs)):\n",
    "        for y in range(len(obs)):\n",
    "            grid[x,y,x,y,0] = 10\n",
    "    return grid\n",
    "\n",
    "experiment_grid = grid(experiment.env)\n",
    "n = 19\n",
    "values = np.zeros((n,n))\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        s = torch.tensor(experiment_grid[x,y], device=DEVICE).to(torch.float32)\n",
    "        v = experiment.value.v(s)\n",
    "        #v = experiment.intrinsic(s.unsqueeze(0).permute(0, 3, 1, 2))\n",
    "        values[x,y] = v.item()\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(values)\n",
    "fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
