{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import FullyObsWrapper, ImgObsWrapper, ReseedWrapper, RGBImgObsWrapper\n",
    "\n",
    "from kitten.common.rng import global_seed, Generator\n",
    "from kitten.nn import Value\n",
    "from kitten.policy import Policy\n",
    "from kitten.experience.collector import GymCollector\n",
    "from kitten.experience.util import build_transition_from_list, build_replay_buffer\n",
    "from kitten.experience import Transitions, AuxiliaryMemoryData\n",
    "from kitten.intrinsic.rnd import RandomNetworkDistillation\n",
    "from kitten.rl.common import td_lambda, monte_carlo_return\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cats.teleport import *\n",
    "from cats.cats import TeleportationResetModule\n",
    "from cats.reset import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_env():\n",
    "    env = gym.make(\"MiniGrid-FourRooms-v0\")\n",
    "    env = FullyObsWrapper(env)\n",
    "    #env = RGBImgObsWrapper(env)\n",
    "    env = ImgObsWrapper(env)\n",
    "    env = ReseedWrapper(env)\n",
    "    env.reset()[0].shape\n",
    "    return env\n",
    "\n",
    "def visualise(env, batch: Transitions):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    draw_map = np.zeros(obs.shape)\n",
    "    white = np.ones(obs.shape)\n",
    "    red = np.zeros(obs.shape)\n",
    "    red[:, :, 0] = 1\n",
    "    green = np.zeros(obs.shape)\n",
    "    green[:, :, 1] = 1\n",
    "    blue = np.zeros(obs.shape)\n",
    "    blue[:, :, 2] = 1\n",
    "\n",
    "    red\n",
    "\n",
    "    # Draw walls\n",
    "    has_wall = np.expand_dims((obs[:, :, 0] == 2), axis=-1)\n",
    "    draw_map += white * has_wall * 0.8\n",
    "    # Draw goal\n",
    "    has_wall = np.expand_dims((obs[:, :, 0] == 8), axis=-1)\n",
    "    draw_map += green * has_wall * 1\n",
    "    \n",
    "    # Draw agent exploration\n",
    "    states = batch.s_0.detach().cpu().numpy() # [batch, a, b, c]\n",
    "    has_agent = states[:, :, :, 0] == 10\n",
    "    has_agent = np.sum(has_agent, axis=0)\n",
    "    has_agent = np.maximum(0,np.log(has_agent))\n",
    "    has_agent = np.expand_dims(has_agent / has_agent.max(), axis=-1)\n",
    "    draw_map += red * has_agent\n",
    "\n",
    "    plt.imshow(draw_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Value Module, Transformations, Policy\n",
    "class MinigridValue(Value):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # We use the CNN from https://minigrid.farama.org/content/training/\n",
    "        # With ReLu changed to LeakyReLu\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Followed by a simple MLP\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.LazyLinear(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LazyLinear(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LazyLinear(1)\n",
    "        )\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return self.linear(self.cnn(x))\n",
    "    \n",
    "    @property\n",
    "    def value(self) -> Value:\n",
    "        return self\n",
    "\n",
    "    def v(self, s: Tensor) -> Tensor:\n",
    "        d = len(s.shape) == 3\n",
    "        if d:\n",
    "            s = s.unsqueeze(0)\n",
    "        if s.shape[-1] == 3:\n",
    "            s = s.permute(0,3,1,2)\n",
    "        v = self.forward(s).squeeze()\n",
    "        return v\n",
    "\n",
    "def build_rnd() -> RandomNetworkDistillation:\n",
    "    def build_net():\n",
    "        return nn.Sequential(\n",
    "        nn.Conv2d(3, 8, (2, 2)),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv2d(8, 16, (2, 2)),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.LazyLinear(128)\n",
    "    )\n",
    "\n",
    "    return RandomNetworkDistillation(\n",
    "        build_net(),\n",
    "        build_net(),\n",
    "        reward_normalisation=True\n",
    "    )\n",
    "\n",
    "class ExplorationPolicy(Policy):\n",
    "    \"\"\" Purely Random Exploration\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, rng: Generator, repeat_probability: float=0.9) -> None:\n",
    "        super().__init__(fn=None)\n",
    "        self.env = env\n",
    "        self.rng = rng\n",
    "        self.p = repeat_probability\n",
    "        self.previous_action = None\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        if self.previous_action is None or self.rng.numpy.random() > self.p:\n",
    "            self.previous_action = self.env.action_space.sample()\n",
    "            return self.previous_action\n",
    "        else:\n",
    "            return self.previous_action\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.previous_action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_random(steps: int = 1000, seed: int = 0):\n",
    "    env = build_env()\n",
    "    rng = global_seed(seed=seed)\n",
    "    policy = ExplorationPolicy(env, rng=rng.build_generator(), repeat_probability=0.5)\n",
    "    collector = GymCollector(policy, env)\n",
    "    batch = build_transition_from_list(collector.collect(steps), DEVICE)\n",
    "    return env, batch\n",
    "\n",
    "def experiment_teleport(steps: int = 10000,  seed: int = 0):\n",
    "    lmbda, gamma = 0.9, 0.99\n",
    "    env = build_env()\n",
    "    rng = global_seed(seed=seed)\n",
    "    policy = ExplorationPolicy(env, rng.build_generator(), repeat_probability=0.5)\n",
    "    intrinsic = build_rnd().to(DEVICE)\n",
    "    value = MinigridValue().to(DEVICE)\n",
    "    optim_value = torch.optim.Adam(value.parameters())\n",
    "    memory, _ = build_replay_buffer(env, capacity=steps)\n",
    "    collector = GymCollector(policy, env, memory=memory)\n",
    "\n",
    "    tm = LatestEpisodeTeleportMemory(rng.build_generator(), device=DEVICE)\n",
    "    rm = ResetMemory(env, capacity=1, rng=rng.build_generator(), device=DEVICE)\n",
    "    ts = EpsilonGreedyTeleport(value, rng.build_generator(), e=0)\n",
    "    trm = TeleportationResetModule(rm, tm, ts)\n",
    "\n",
    "    log = {\"value_loss\": [], \"return\": []}\n",
    "\n",
    "    tm.reset(collector.env, collector.obs)\n",
    "    step = 0\n",
    "    while step < steps:\n",
    "        # Collection Phase\n",
    "        batch = []\n",
    "        while True:\n",
    "            data = list(collector.collect(n=1)[-1])\n",
    "            memory.append(data)\n",
    "            batch.append(data)\n",
    "            tm.update(collector.env, obs=data[0])\n",
    "            if data[-2] or data[-1]:\n",
    "                # Terminated or truncated\n",
    "                break\n",
    "\n",
    "        batch = build_transition_from_list(batch, device=DEVICE)\n",
    "        step += batch.shape[0]\n",
    "\n",
    "        # Manually reshape and scale\n",
    "        batch.s_0 = batch.s_0.permute(0,3,1,2)\n",
    "        batch.s_1 = batch.s_1.permute(0,3,1,2)\n",
    "\n",
    "        # Reset\n",
    "        trm.select(collector)\n",
    "\n",
    "        # Intrinsic\n",
    "            # Update\n",
    "        intrinsic.update(batch, aux=AuxiliaryMemoryData.placeholder(batch), step=step)\n",
    "            # Override reward\n",
    "\n",
    "        r_t, r_e, r_i = intrinsic.reward(batch)\n",
    "        batch.r = r_i\n",
    "        # Update Value Function\n",
    "        value_targets = td_lambda(batch, lmbda, gamma, value)\n",
    "        total_value_loss = 0\n",
    "        for _ in range(8):\n",
    "            optim_value.zero_grad()\n",
    "            value_loss = ((value_targets - value.v(batch.s_0))**2).mean()\n",
    "            total_value_loss += value_loss.item()\n",
    "            value_loss.backward()\n",
    "            optim_value.step()\n",
    "\n",
    "        # Logging\n",
    "        log[\"return\"].append(value_targets[0].item())\n",
    "        log[\"value_loss\"].append(total_value_loss)\n",
    "\n",
    "    return env, memory.sample(steps)[0], value, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, batch, value, log = experiment_teleport(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(env, batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
