{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from kitten.common.rng import global_seed\n",
    "from kitten.experience.collector import GymCollector\n",
    "from kitten.experience.util import build_transition_from_list\n",
    "from kitten.experience import Transitions, AuxiliaryMemoryData\n",
    "from kitten.rl.common import td_lambda, generate_minibatches\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from cats.agent.policy import ExplorationPolicy\n",
    "from cats.agent.minigrid.value import MinigridValue\n",
    "from cats.agent.experiment import ExperimentBase, build_env\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def visualise(ax,\n",
    "              env,\n",
    "              batch: Transitions | list[np.ndarray],\n",
    "              density: bool = True,\n",
    "):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    draw_map = np.zeros(obs.shape)\n",
    "    white = np.ones(obs.shape)\n",
    "    red = np.zeros(obs.shape)\n",
    "    red[:, :, 0] = 1\n",
    "    green = np.zeros(obs.shape)\n",
    "    green[:, :, 1] = 1\n",
    "    blue = np.zeros(obs.shape)\n",
    "    blue[:, :, 2] = 1\n",
    "\n",
    "    # Draw walls\n",
    "    has_wall = np.expand_dims((obs[:, :, 0] == 2), axis=-1)\n",
    "    draw_map += white * has_wall * 0.8\n",
    "    # Draw goal\n",
    "    has_wall = np.expand_dims((obs[:, :, 0] == 8), axis=-1)\n",
    "    draw_map += green * has_wall * 1\n",
    "\n",
    "    # Draw agent exploration\n",
    "    if isinstance(batch, Transitions):\n",
    "        states = batch.s_0.detach().cpu().numpy()  # [batch, a, b, c]\n",
    "    else:\n",
    "        states = np.array(batch)\n",
    "    has_agent = states[:, :, :, 0] == 10\n",
    "    has_agent = np.sum(has_agent, axis=0)\n",
    "    #has_agent = np.maximum(0, np.log(has_agent))\n",
    "    has_agent = np.expand_dims(has_agent / has_agent.max(), axis=-1)\n",
    "    if not density:\n",
    "        has_agent = (has_agent>0)\n",
    "    draw_map += red * has_agent\n",
    "\n",
    "    ax.imshow(draw_map)\n",
    "\n",
    "def grid(env, dir: int =0):\n",
    "    obs, _ = env.reset()\n",
    "    has_agent = obs[:, :, 0] == 10\n",
    "    obs[:, :, 0] -= (has_agent * 9).astype(np.uint8)\n",
    "    obs[:, :, 2] = 0\n",
    "    grid = np.reshape(obs, (1,1, *obs.shape))\n",
    "    grid = np.repeat(grid, len(obs), 0)\n",
    "    grid = np.repeat(grid, len(obs), 1)\n",
    "    for x in range(len(obs)):\n",
    "        for y in range(len(obs)):\n",
    "            grid[x,y,x,y,0] = 10\n",
    "            grid[x,y,x,y,2] = dir\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_update_epochs = 4\n",
    "minibatch_size = 32\n",
    "\n",
    "class TeleportStrategyExperiment(ExperimentBase):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: DictConfig,\n",
    "        deprecated_testing_flag: bool = False,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__(cfg,\n",
    "                         normalise_obs=False,\n",
    "                         deprecated_testing_flag=False,\n",
    "                         device=device)\n",
    "        self._gamma: float = cfg.algorithm.gamma\n",
    "        self._lmbda: float = cfg.algorithm.lmbda\n",
    "\n",
    "\n",
    "    def _build_policy(self) -> None:\n",
    "        # Random Policy\n",
    "        self._policy = ExplorationPolicy(\n",
    "            self.env, self.rng.build_generator(), repeat_probability=self.cfg.policy.p\n",
    "        )\n",
    "        # Build Value Estimator\n",
    "        self.value = MinigridValue().to(DEVICE)\n",
    "        self.optim_v = torch.optim.Adam(params=self.value.parameters()) \n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self._policy\n",
    "    \n",
    "    @property\n",
    "    def value_container(self):\n",
    "        return self.value\n",
    "\n",
    "    def run(self):\n",
    "        self.tm.reset(self.collector.env, self.collector.env)\n",
    "        step, steps = 0, self.cfg.train.total_frames\n",
    "        while step < steps:\n",
    "            \n",
    "            # Collect batch\n",
    "            batch_ = []\n",
    "            goal_step = step + 100 # TODO\n",
    "            while step < goal_step:\n",
    "                step += 1\n",
    "                data = self.collector.collect(n=1)[-1]\n",
    "                batch_.append(data)\n",
    "                self.tm.update(self.collector.env, obs=data[0])\n",
    "                if data[-2] or data[-1]: # Terminated or truncated\n",
    "                    break\n",
    "            batch = build_transition_from_list(batch_, device=DEVICE)\n",
    "\n",
    "            # Reshape for ConvNet\n",
    "            batch.s_0 = batch.s_0.permute(0, 3, 1, 2)\n",
    "            batch.s_1 = batch.s_1.permute(0, 3, 1, 2)\n",
    "\n",
    "            # Intrinsic Update\n",
    "            for _ in range(n_update_epochs):\n",
    "                for i, mb in generate_minibatches(batch, mb_size=minibatch_size, rng=self.rng):\n",
    "                    self.intrinsic.update(mb, aux=AuxiliaryMemoryData.placeholder(mb), step=step)\n",
    "            \n",
    "            # Override rewards\n",
    "            _, _, r_i = self.intrinsic.reward(batch)\n",
    "                # Temp?\n",
    "            batch.r = r_i  -1\n",
    "\n",
    "            # Value Update\n",
    "            value_targets = td_lambda(batch, self._lmbda, self._gamma, self.value)\n",
    "            total_value_loss = 0\n",
    "            for _ in range(n_update_epochs):\n",
    "                for i, mb in generate_minibatches(value_targets, mb_size=minibatch_size, rng=self.rng):\n",
    "                    self.optim_v.zero_grad()\n",
    "                    value_loss = ((mb - self.value.v(batch.s_0[i])) ** 2).mean()\n",
    "                    total_value_loss += value_loss.item()\n",
    "                    value_loss.backward()\n",
    "                    self.optim_v.step()\n",
    "\n",
    "\n",
    "            # Always reset? Or only reset on truncation?\n",
    "            self._reset(batch_[-1][0], batch_[-1][4])\n",
    "\n",
    "            self.logger.log({\"value_loss\": total_value_loss})\n",
    "\n",
    "def experiment_random(steps: int = 10000, seed: int = 0):\n",
    "    with initialize(version_base=None, config_path=\"cats/config\"):\n",
    "        cfg = compose(\n",
    "            config_name=\"defaults_online.yaml\",\n",
    "            overrides=[\n",
    "                f\"seed={seed}\",\n",
    "                \"cats.fixed_reset=true\",\n",
    "                \"cats.teleport.enable=true\",\n",
    "            ],\n",
    "        )\n",
    "    env = build_env(cfg)\n",
    "    rng = global_seed(seed=seed)\n",
    "    policy = ExplorationPolicy(env, rng=rng.build_generator(), repeat_probability=0.5)\n",
    "    collector = GymCollector(policy, env)\n",
    "    batch = build_transition_from_list(collector.collect(steps), DEVICE)\n",
    "    return env, batch\n",
    "\n",
    "\n",
    "def experiment_teleport(steps: int = 10000, seed: int = 0):\n",
    "    with initialize(version_base=None, config_path=\"cats/config\"):\n",
    "        cfg = compose(\n",
    "            config_name=\"defaults_online.yaml\",\n",
    "            overrides=[\n",
    "                f\"seed={seed}\",\n",
    "                f\"train.total_frames={steps}\",\n",
    "                \"cats.fixed_reset=true\",\n",
    "                \"cats.teleport.enable=true\",\n",
    "                f\"env.max_steps={steps}\"\n",
    "            ],\n",
    "        )\n",
    "    experiment = TeleportStrategyExperiment(cfg, device=DEVICE)\n",
    "    experiment.run()    \n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiment_teleport(steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3)\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "env, batch = experiment.env, experiment.memory.sample(len(experiment.memory))[0]\n",
    "\n",
    "visualise(\n",
    "    ax = axs[0][0],\n",
    "    env=env,\n",
    "    batch=batch,\n",
    "    density=False\n",
    ")\n",
    "\n",
    "visualise(\n",
    "    ax = axs[0][1],\n",
    "    env=env,\n",
    "    batch=batch,\n",
    "    density=True\n",
    ")\n",
    "\n",
    "visualise(\n",
    "    ax = axs[0][2],\n",
    "    env= env,\n",
    "    batch=experiment.logger._engine.results['reset_obs'],\n",
    ")\n",
    "\n",
    "experiment_grid = grid(experiment.env)\n",
    "n = 19\n",
    "values_v = np.zeros((n,n))\n",
    "values_r = np.zeros((n,n))\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        s = torch.tensor(experiment_grid[x,y], device=DEVICE).to(torch.float32)\n",
    "        s = s.permute((2,0,1))\n",
    "        v_1 = experiment.value.v(s)\n",
    "        v_2 = experiment.intrinsic(s.unsqueeze(0))\n",
    "        values_v[x,y] = v_1.item()\n",
    "        values_r[x,y] = v_2.item()\n",
    "# Value Function\n",
    "im = axs[1][0].imshow(values_v)\n",
    "fig.colorbar(im, ax=axs[1][0])\n",
    "# Intrinsic Reward\n",
    "im = axs[1][1].imshow(values_r)\n",
    "fig.colorbar(im, ax=axs[1][1])\n",
    "\n",
    "axs[1][2].plot(experiment.logger._engine.results[\"value_loss\"])\n",
    "axs[1][2].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, batch = experiment_random(steps=2048, seed=0)\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "fig.set_size_inches(4, 8)\n",
    "visualise(\n",
    "    ax = axs[0],\n",
    "    env=env,\n",
    "    batch=batch,\n",
    "    density=False\n",
    ")\n",
    "\n",
    "visualise(\n",
    "    ax = axs[1],\n",
    "    env=env,\n",
    "    batch=batch,\n",
    "    density=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "plt.imshow(obs[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(obs[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minigrid.core.constants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
