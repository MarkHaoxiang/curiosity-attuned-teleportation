{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Disentanglement \n",
    "\n",
    "Disentangling Epistemic and Aleatoric Uncertainty in\n",
    "Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Generate Training Data\n",
    "N, M, D, A = 1000, 10, 5, 0.05\n",
    "\n",
    "X = (10 - torch.linspace(0, 100, N) ** 0.5).sort()[0].unsqueeze(1)\n",
    "Y = torch.sin(X) + torch.randn_like(X) * A * X\n",
    "# Y = (torch.sin(X)+torch.randn_like(X) * A)\n",
    "\n",
    "X_all = torch.linspace(0, D * 3, 1000).unsqueeze(1)\n",
    "Y_all = torch.sin(X_all)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y)\n",
    "ax.set_title(\"Toy Sinusoid\")\n",
    "ax.plot(X_all, Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disagreement with MSE prediction\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N_ENSEMBLE = 5\n",
    "N_EPOCH = 1000\n",
    "\n",
    "mse_ensemble = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 32), nn.Tanh(), nn.Linear(32, 32), nn.Tanh(), nn.Linear(32, 1)\n",
    "    )\n",
    "    for _ in range(N_ENSEMBLE)\n",
    "]\n",
    "\n",
    "optim_mse = torch.optim.Adam(nn.ModuleList(mse_ensemble).parameters(), lr=0.01)\n",
    "\n",
    "debug_loss = []\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCH)):\n",
    "    batch = torch.randint(0, N, size=(32,))\n",
    "    Y_p = torch.stack([e(X[batch]) for e in mse_ensemble])\n",
    "    optim_mse.zero_grad()\n",
    "    loss = torch.mean((Y_p - Y[batch].unsqueeze(0)) ** 2)\n",
    "    loss.backward()\n",
    "    debug_loss.append(loss.item())\n",
    "    optim_mse.step()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 3)\n",
    "# Training Loss\n",
    "axs[0].plot(debug_loss)\n",
    "axs[0].set_title(\"Training Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "\n",
    "Y_p = torch.stack([e(X_all) for e in mse_ensemble]).squeeze()\n",
    "variance = torch.var(Y_p, dim=0).detach().cpu().numpy()\n",
    "mean = torch.mean(Y_p, dim=0).detach().cpu().numpy()\n",
    "axs[1].scatter(X.squeeze(), Y.squeeze(), color=\"b\")\n",
    "for y_p in Y_p:\n",
    "    axs[1].plot(X_all, y_p.detach().cpu().numpy(), \"--\")\n",
    "axs[1].plot(X_all, Y_all, color=\"b\")\n",
    "axs[1].plot(X_all, mean, color=\"r\")\n",
    "axs[1].fill_between(X_all.squeeze(), mean - variance**0.5, mean + variance**0.5)\n",
    "axs[1].set_title(\"Epistemic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disagreement with NLL prediction\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N_ENSEMBLE = 5\n",
    "N_EPOCH = 1000\n",
    "\n",
    "nll_ensemble = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 32), nn.Tanh(), nn.Linear(32, 32), nn.Tanh(), nn.Linear(32, 2)\n",
    "    )\n",
    "    for _ in range(N_ENSEMBLE)\n",
    "]\n",
    "\n",
    "optim_nll = torch.optim.Adam(nn.ModuleList(nll_ensemble).parameters(), lr=0.001)\n",
    "softplus = nn.Softplus()\n",
    "\n",
    "debug_loss = []\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCH)):\n",
    "    batch = torch.randint(0, N, size=(32,))\n",
    "    Y_p = torch.stack([e(X[batch]) for e in nll_ensemble])\n",
    "    mu = Y_p[:, :, 0].unsqueeze(-1)\n",
    "    sigma = softplus(Y_p[:, :, 1].unsqueeze(-1))\n",
    "    optim_nll.zero_grad()\n",
    "    loss = torch.mean(\n",
    "        torch.log(sigma) / 2 + (Y[batch].unsqueeze(0) - mu) ** 2 / (2 * sigma)\n",
    "    )\n",
    "    loss.backward()\n",
    "    debug_loss.append(loss.item())\n",
    "    optim_nll.step()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_size_inches(15, 3)\n",
    "# Training Loss\n",
    "axs[0].plot(debug_loss)\n",
    "axs[0].set_title(\"Training Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "\n",
    "Y_p = torch.stack([e(X_all) for e in nll_ensemble])[:, :, 0].squeeze()\n",
    "aleatoric = (\n",
    "    torch.mean(\n",
    "        torch.stack([softplus(e(X_all)) for e in nll_ensemble])[:, :, 1].squeeze(),\n",
    "        dim=0,\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "variance = torch.var(Y_p, dim=0).detach().cpu().numpy()\n",
    "mean = torch.mean(Y_p, dim=0).detach().cpu().numpy()\n",
    "axs[1].scatter(X.squeeze(), Y.squeeze(), color=\"b\")\n",
    "for y_p in Y_p:\n",
    "    axs[1].plot(X_all, y_p.detach().cpu().numpy(), \"--\")\n",
    "axs[1].plot(X_all, Y_all, color=\"b\")\n",
    "axs[1].plot(X_all, mean, color=\"r\")\n",
    "axs[1].fill_between(\n",
    "    X_all.squeeze(), mean - variance**0.5, mean + variance**0.5, alpha=0.5\n",
    ")\n",
    "axs[1].set_title(\"Epistemic\")\n",
    "\n",
    "\n",
    "axs[2].plot(X_all, mean)\n",
    "axs[2].fill_between(\n",
    "    X_all.squeeze(), mean - aleatoric**0.5, mean + aleatoric**0.5, alpha=0.5\n",
    ")\n",
    "axs[2].set_title(\"Aleatoric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disagreement with bNLL prediction\n",
    "torch.manual_seed(1)\n",
    "\n",
    "N_ENSEMBLE = 5\n",
    "N_EPOCH = 1000\n",
    "\n",
    "bnll_ensemble = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(1, 32), nn.Tanh(), nn.Linear(32, 32), nn.Tanh(), nn.Linear(32, 2)\n",
    "    )\n",
    "    for _ in range(N_ENSEMBLE)\n",
    "]\n",
    "\n",
    "beta = 0.5\n",
    "optim_bnll = torch.optim.Adam(nn.ModuleList(bnll_ensemble).parameters(), lr=0.01)\n",
    "softplus = nn.Softplus()\n",
    "\n",
    "debug_loss = []\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCH)):\n",
    "    batch = torch.randint(0, N, size=(32,))\n",
    "    Y_p = torch.stack([e(X[batch]) for e in bnll_ensemble])\n",
    "    mu = Y_p[:, :, 0].unsqueeze(-1)\n",
    "    sigma = softplus(Y_p[:, :, 1].unsqueeze(-1))\n",
    "    optim_bnll.zero_grad()\n",
    "    sigma_beta = (sigma**beta).detach()\n",
    "    loss = torch.log(sigma) / 2 + (Y[batch].unsqueeze(0) - mu) ** 2 / (2 * sigma)\n",
    "    loss = loss * sigma_beta\n",
    "    loss = torch.mean(loss)\n",
    "    loss.backward()\n",
    "    debug_loss.append(loss.item())\n",
    "    optim_bnll.step()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_size_inches(15, 3)\n",
    "# Training Loss\n",
    "axs[0].plot(debug_loss)\n",
    "axs[0].set_title(\"Training Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "\n",
    "Y_p = torch.stack([e(X_all) for e in bnll_ensemble])[:, :, 0].squeeze()\n",
    "aleatoric = (\n",
    "    torch.mean(\n",
    "        torch.stack([softplus(e(X_all)) for e in bnll_ensemble])[:, :, 1].squeeze(),\n",
    "        dim=0,\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "variance = torch.var(Y_p, dim=0).detach().cpu().numpy()\n",
    "mean = torch.mean(Y_p, dim=0).detach().cpu().numpy()\n",
    "axs[1].scatter(X.squeeze(), Y.squeeze(), color=\"b\")\n",
    "for y_p in Y_p:\n",
    "    axs[1].plot(X_all, y_p.detach().cpu().numpy(), \"--\")\n",
    "axs[1].plot(X_all, Y_all, color=\"b\")\n",
    "axs[1].plot(X_all, mean, color=\"r\")\n",
    "axs[1].fill_between(\n",
    "    X_all.squeeze(), mean - variance**0.5, mean + variance**0.5, alpha=0.5\n",
    ")\n",
    "axs[1].set_title(\"Epistemic\")\n",
    "\n",
    "axs[2].plot(X_all, mean)\n",
    "axs[2].fill_between(\n",
    "    X_all.squeeze(), mean - aleatoric**0.5, mean + aleatoric**0.5, alpha=0.5\n",
    ")\n",
    "axs[2].set_title(\"Aleatoric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
